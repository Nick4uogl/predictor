# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RR4JuRArgAd79ONBv6fLGQXYAOLLe3hT
"""

# Install required packages
!pip install torch pandas numpy scikit-learn joblib

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import Dataset, DataLoader
import logging
import os
import joblib
from sklearn.model_selection import train_test_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from scipy import stats

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Upload your CSV file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
csv_file = list(uploaded.keys())[0]
logger.info(f"Uploaded file: {csv_file}")

# Model parameters
SEQUENCE_LENGTH = 24  # Number of hours to look back
PREDICTION_HORIZON = 24  # Number of hours to predict ahead
HIDDEN_SIZE = 256  # Increased hidden size
NUM_LAYERS = 3  # Increased number of layers
LEARNING_RATE = 0.001
NUM_EPOCHS = 200  # Increased epochs
BATCH_SIZE = 32
DROPOUT_RATE = 0.2
PATIENCE = 20  # For early stopping

class AirQualityDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor(self.targets[idx])

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, prediction_horizon, dropout_rate=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.prediction_horizon = prediction_horizon
        self.output_size = output_size
        self.dropout_rate = dropout_rate

        # LSTM layers with dropout
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0
        )
        
        # Batch normalization
        self.batch_norm = nn.BatchNorm1d(hidden_size)
        
        # Additional fully connected layers with dropout
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size // 2, output_size * prediction_horizon)
        
        # Activation functions
        self.relu = nn.ReLU()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # LSTM forward pass
        out, _ = self.lstm(x, (h0, c0))
        
        # Get the last time step
        out = out[:, -1, :]
        
        # Apply batch normalization
        out = self.batch_norm(out)
        
        # Additional fully connected layers
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.fc2(out)
        
        # Reshape to [batch_size, prediction_horizon, output_size]
        out = out.view(out.size(0), self.prediction_horizon, self.output_size)
        return out

def remove_outliers(df, columns, z_threshold=3):
    """Remove outliers using z-score method"""
    df_clean = df.copy()
    for col in columns:
        z_scores = np.abs(stats.zscore(df_clean[col]))
        df_clean = df_clean[z_scores < z_threshold]
    return df_clean

def prepare_data(csv_file):
    """Prepare data for LSTM training with improved preprocessing"""
    # Read CSV file
    df = pd.read_csv(csv_file)

    # Convert timestamp to datetime and set as index
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)

    # Handle missing values and outliers
    air_quality_columns = ['co', 'no2', 'o3', 'so2', 'pm25', 'pm10']
    
    # Remove outliers
    df = remove_outliers(df, air_quality_columns)
    
    # Handle missing values with more sophisticated method
    for col in air_quality_columns:
        # Forward fill
        df[col] = df[col].fillna(method='ffill')
        # Backward fill for any remaining NaNs
        df[col] = df[col].fillna(method='bfill')
        # If still any NaNs, fill with median
        df[col] = df[col].fillna(df[col].median())

    # Normalize the data
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df[air_quality_columns])

    # Create sequences
    X, y = [], []
    for i in range(len(scaled_data) - SEQUENCE_LENGTH - PREDICTION_HORIZON + 1):
        X.append(scaled_data[i:(i + SEQUENCE_LENGTH)])
        y.append(scaled_data[i + SEQUENCE_LENGTH:i + SEQUENCE_LENGTH + PREDICTION_HORIZON])

    return np.array(X), np.array(y), scaler, df[air_quality_columns]

def train_model(csv_file, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE):
    """Train LSTM model with improved training process"""
    # Prepare data
    X, y, scaler, original_data = prepare_data(csv_file)

    # Split into train, validation, and test sets
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)

    # Create datasets and dataloaders
    train_dataset = AirQualityDataset(X_train, y_train)
    val_dataset = AirQualityDataset(X_val, y_val)
    test_dataset = AirQualityDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Initialize model
    input_size = 6  # co, no2, o3, so2, pm25, pm10
    hidden_size = HIDDEN_SIZE
    num_layers = NUM_LAYERS
    output_size = 6  # Predicting all air quality parameters

    model = LSTMModel(input_size, hidden_size, num_layers, output_size, PREDICTION_HORIZON, DROPOUT_RATE)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    # Learning rate scheduler
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)
    
    # Early stopping variables
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                outputs = model(batch_X)
                val_loss += criterion(outputs, batch_y).item()

        # Update learning rate
        scheduler.step(val_loss)

        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1

        if (epoch + 1) % 10 == 0:
            logger.info(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')

        if patience_counter >= PATIENCE:
            logger.info(f'Early stopping triggered at epoch {epoch+1}')
            break

    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    return model, scaler, original_data

def predict(model, scaler, data, hours_ahead=72):
    """Make predictions for all parameters"""
    model.eval()
    with torch.no_grad():
        # Prepare initial input sequence
        input_data = data.values[-SEQUENCE_LENGTH:]
        input_data = scaler.transform(input_data)
        current_sequence = input_data.copy()
        
        # Initialize predictions list
        all_predictions = []
        
        # Make predictions recursively
        for _ in range(hours_ahead):
            # Prepare input tensor
            input_tensor = torch.FloatTensor(current_sequence).unsqueeze(0)
            
            # Make prediction
            output = model(input_tensor)
            # Get the first prediction (next hour)
            next_pred = output.squeeze().numpy()[0]
            
            # Add prediction to results
            all_predictions.append(next_pred)
            
            # Update sequence for next prediction
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1] = next_pred
        
        # Convert predictions to numpy array
        predictions = np.array(all_predictions)
        
        # Create a dummy array for inverse transform
        dummy_pred = np.zeros((len(predictions), len(scaler.feature_names_in_)))
        dummy_pred[:, :predictions.shape[1]] = predictions
        
        # Inverse transform predictions
        predictions = scaler.inverse_transform(dummy_pred)
        
        # Create timestamps for the predictions
        last_timestamp = data.index[-1]
        future_times = [last_timestamp + pd.Timedelta(hours=i+1) for i in range(len(predictions))]
        
        # Create DataFrame with predictions
        result = pd.DataFrame(
            predictions,
            index=future_times,
            columns=scaler.feature_names_in_
        )
        
        return result

def save_model(model, scaler, city):
    """Save trained model and scaler"""
    os.makedirs('models', exist_ok=True)
    torch.save(model.state_dict(), f'models/{city}_lstm_model.pth')
    joblib.dump(scaler, f'models/{city}_lstm_scaler.pkl')

# Train model using the uploaded CSV file
city = csv_file.split('_')[0].capitalize()  # Extract city name from filename
logger.info(f"Training model for {city}")
model, scaler, original_data = train_model(csv_file)

# Get predictions
predictions = predict(model, scaler, original_data)

# Calculate and log RMSE for each parameter
param_indices = {'co': 0, 'no2': 1, 'o3': 2, 'so2': 3, 'pm25': 4, 'pm10': 5}
for param, index in param_indices.items():
    actual_values = original_data[param].values[-len(predictions):]
    rmse = np.sqrt(np.mean((predictions[:, index] - actual_values) ** 2))
    logger.info(f"RMSE for {param} in {city}: {rmse:.4f}")

# Save the model
save_model(model, scaler, city)
logger.info(f"Model for {city} saved")

# Download the trained model and scaler
from google.colab import files
files.download(f'models/{city}_lstm_model.pth')
files.download(f'models/{city}_lstm_scaler.pkl')